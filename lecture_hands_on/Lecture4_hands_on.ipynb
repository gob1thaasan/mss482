{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6027a45f-d731-4ff1-8527-b4db811bb2f9",
   "metadata": {
    "id": "fRuRcAt3F_wS"
   },
   "source": [
    "\n",
    "# MSS482 - GRAPHING TECHNOLOGY IN MATHEMATICS AND SCIENCE\n",
    "\n",
    "**SEMESTER 1 2023/2024**\n",
    "\n",
    "\n",
    ">R.U.Gobithaasan (2023). School of Mathematical Sciences, Universiti Sains Malaysia.\n",
    "[Official Website](https://math.usm.my/academic-profile/705-gobithaasan-rudrusamy) \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "     © 2023 R.U. Gobithaasan All Rights Reserved.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9069ae-3da6-41b9-88f1-cb26da15581c",
   "metadata": {},
   "source": [
    "# Scatterplots, Linear & Multiple Regression. \n",
    "- https://www.pythonfordatascience.org/linear-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82e975-28ad-49ac-9960-82538ce11fd0",
   "metadata": {},
   "source": [
    "4.1 Scatterplot <br>\n",
    "\n",
    "4.2. Linear Regression <br>\n",
    "\n",
    "4.3 Multiple Regression <br>\n",
    "\n",
    "4.3 Using Seaborn for Regression Visualization <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ade7d",
   "metadata": {},
   "source": [
    "### requirements\n",
    "\n",
    "> Install the following: `!python -m pip install pandas`\n",
    "1. pandas\n",
    "2. researchpy\n",
    "3. statsmodels\n",
    "4. matplotlib\n",
    "5. seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aad3b9-a5ff-430c-bda0-922d8e3de635",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset: Online Dataset sources\n",
    "\n",
    "**Online Sources:** \n",
    "- Google Dataset Search: https://datasetsearch.research.google.com/ \n",
    "- Kaggle: https://www.kaggle.com/datasets \n",
    "- UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets.php \n",
    "- Earth Data: https://www.earthdata.nasa.gov/\n",
    "- Scikit Dataset: https://scikit-learn.org/stable/datasets.html\n",
    "- https://github.com/gob1thaasan/Data-sets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0cf54e",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba723a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic command to display Matplotlib plots inline :https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%matplotlib inline\n",
    "# To ignore warnings, use the following code to make the display more attractive.\n",
    "# Import seaborn and matplotlib.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa413f35",
   "metadata": {},
   "source": [
    "# Scatter plot: a part of data visualization\n",
    "- https://www.kaggle.com/code/benhamner/python-data-visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = pd.read_csv(\"../data/iris.csv\")\n",
    "\n",
    "# Selecting only numeric columns for correlation calculation\n",
    "feature_columns = iris.drop(['species', 'species_id'], axis=1)\n",
    "\n",
    "# Calculating the correlation matrix\n",
    "corr_matrix = feature_columns.corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "# Plotting a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 10})\n",
    "plt.title('Correlation Heatmap of Iris Dataset ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f39c6",
   "metadata": {},
   "source": [
    ">Let's see how many instances we have of each species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"species\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d16655",
   "metadata": {},
   "source": [
    "> The most correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot(kind=\"scatter\", x=\"petal_length\", y=\"petal_width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba168c",
   "metadata": {},
   "source": [
    "A FacetGrid is a multi-axes grid with subplots visualizing the distribution of variables of a dataset and the relationship between multiple variables.\n",
    "- https://seaborn.pydata.org/generated/seaborn.FacetGrid.html \n",
    "- The first line initializes the grid and color based on the species, but doesn’t plot anything on it.\n",
    "- the `map` function plots two features in the grid space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a773ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(iris, hue=\"species\").\\\n",
    "    map(plt.scatter, \"petal_length\", \"petal_width\").\\\n",
    "        add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d58b33",
   "metadata": {},
   "source": [
    ">The least correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure\n",
    "sns.jointplot(x=\"sepal_length\", y=\"sepal_width\", data=iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd4f04",
   "metadata": {},
   "source": [
    "> Let's investigate further the distribution of linearly correlated features visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9315111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at an individual feature in Seaborn through a boxplot\n",
    "sns.boxplot(x=\"species\", y=\"petal_length\", data=iris, hue=\"species\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3421ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A final seaborn plot useful for looking at univariate relations is the kdeplot,\n",
    "# which creates and visualizes a kernel density estimate of the underlying feature\n",
    "sns.FacetGrid(iris, hue=\"species\").map(sns.kdeplot, \"petal_length\").add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572f7c7",
   "metadata": {},
   "source": [
    ">A violin plot combines boxplot and its distribution curves.\n",
    "- Denser regions of the data are fatter, and sparser thinner in a violin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"species\", y=\"petal_length\", data=iris, hue=\"species\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3abc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris.drop(['species_id'], axis=1), hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88a9bf",
   "metadata": {},
   "source": [
    "> scatterplot in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "x = iris['sepal_length']\n",
    "y = iris['petal_width']\n",
    "z = iris['petal_length']\n",
    "\n",
    "ax.set_xlabel(\"sepal_length\")\n",
    "ax.set_ylabel(\"petal_width\")\n",
    "ax.set_zlabel(\"petal_length\")\n",
    "\n",
    "ax.scatter(x, y, z,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ddc00",
   "metadata": {},
   "source": [
    "> install the following\n",
    "- `python -m pip install plotly`\n",
    "- `python -m pip install nbformat`\n",
    "- `python -m pip install ipykernel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "fig = px.scatter_3d(df, x='sepal_length',  y='petal_length', z='petal_width',color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8fb3a",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Regression\n",
    "\n",
    "**Linear regression is a fundamental statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (predictors) by fitting a linear equation to observed data**. \n",
    "\n",
    "- It assumes that there is a linear relationship between the variables and aims to find the best-fitting straight line that describes the relationship.\n",
    "\n",
    "The equation of a simple linear regression with one independent variable can be represented as:\n",
    "\n",
    "$ y = mx + c $\n",
    "\n",
    "Where:\n",
    "- $ y $ is the dependent variable (the variable we want to predict). Alsk called as response variable.\n",
    "- $ x $ is the independent variable or known as the predictor (the variable used to make predictions).\n",
    "- $ m $ is the slope of the line (the effect of $ x $ on $ y $).\n",
    "- $ c $ is the y-intercept (the value of $ y $ when $ x $ is 0).\n",
    "\n",
    "> In statistics, we usually denote linear regression model as: $ y = \\beta_0 + \\beta_1 x $\n",
    "\n",
    "The goal of linear regression is to find the values of $ m $ and $ c $ that minimize the difference between the predicted values and the actual values in the dataset. This is often done using a method called the least squares method, which minimizes the sum of the squared differences between the observed and predicted values.\n",
    "\n",
    "\n",
    "Linear regression is widely used for various purposes, including prediction, forecasting, and understanding the relationship between variables in many fields such as economics, finance, biology, and social sciences.\n",
    "\n",
    "\n",
    "We can carry regression analysis with two types of packages in Python:\n",
    "\n",
    "1. `statsmodels`: is more focused on statistical inference, detailed analysis, and hypothesis testing, providing statistical summaries.\n",
    "2. `scikit-learn`: geared towards predictive modeling, offering a broader range of machine learning algorithms and evaluation metrics.\n",
    "\n",
    "The choice between them depends on the specific requirements of your analysis—whether you need detailed statistical insights or are focused on predictive performance and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0315c",
   "metadata": {},
   "source": [
    "> this is a controlled experiment with know coeeficients; $\\beta_0 = 3$ and $\\beta_1 = 4$, hence\n",
    "\n",
    "$y = 3 + 4 * X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab1b3a",
   "metadata": {},
   "source": [
    "### ASSUMPTIONS\n",
    "source: [ACAD EMERG MED d January 2004, Vol. 11, No. 1 d www.aemj.org](https://onlinelibrary.wiley.com/doi/pdf/10.1197/j.aem.2003.09.005)\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/regression_assumption.png\" alt=\"Four Assumptions of Regression\" width=\"300\" height=\"300\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "1. **There is some linear relationship between the predictor and outcome variable.** As the values of the points increase along the x-axis, their values along the y-axis also increase. The cloud of points seems to center around a straight line rather than a curve or other shape.\n",
    "\n",
    "2. **The variation around the regression line is constant (homoscedasticity)**. Some points may be farther from the regression line than others. Homoscedasticity means that as the eye moves laterally along the x-axis, the average variation of the points from the regression line stays about the same.\n",
    "\n",
    "3. **The variation of the data around the regression line follows a normal distribution at all values of the predictor variable.** If one examines the data points at any particular value of x, they will form a bell-shaped or normal curve around the value of the regression line at that point. The majority of points will be close to the regression line, and fewer points will be farther away.\n",
    "\n",
    "4. **The deviation of each data point from the regression line is independent of the deviation of the other data points.** The value of one point and its relationship to the regression line has no relation- ship or bearing on the value of another point in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a3d82",
   "metadata": {},
   "source": [
    "> using `statsmodels `   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "# adding noise to the dependent variable\n",
    "y = 3 + 4 * X + 0.01*np.random.randn(100, 1)\n",
    "\n",
    "# Add constant term for intercept in statsmodels\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary statistics\n",
    "print(model.summary())\n",
    "\n",
    "# Plotting the data and regression line\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], model.predict(X), color='red', linewidth=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression with Statsmodels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2143f",
   "metadata": {},
   "source": [
    "The output of the `statsmodels` linear regression summary provides various statistical measures and information about the fitted model. Here's how to interpret some of the key components of the summary:\n",
    "\n",
    "- Dependent Variable: Indicates the dependent variable used in the model.\n",
    "- Model: The model type and method used (e.g., Ordinary Least Squares - OLS).\n",
    "    - OLS aims to find the line (in the case of simple linear regression) or the plane/hyperplane (in multiple linear regression) that minimizes the sum of the squared differences between the observed and predicted values of the dependent variable. It does so by finding the coefficients for the independent variables that best fit the data by minimizing the sum of the squares of the vertical distances (residuals) between the observed data points and the predicted values from the regression line/plane.\n",
    "- Method: The method used for estimating the parameters (e.g., Least Squares).\n",
    "- Date and Time: When the model was fitted.\n",
    "- Number of observations: The number of data points used in the model.\n",
    "\n",
    "Coefficients Table:\n",
    "-  coef: The estimated coefficients (slopes) for each independent variable.\n",
    "- std err: The standard errors of the coefficients.\n",
    "- t: The t-statistic value for testing the null hypothesis that the coefficient is zero.\n",
    "    - The formula for the t-statistic is $t=\\frac{coef}{std \\ err}$\n",
    "- P>|t|: The p-values associated with the t-statistics. These indicate the significance of each variable. Smaller p-values suggest more significance.\n",
    "- [0.025, 0.975]: The 95% confidence intervals for the coefficients.\n",
    "\n",
    "Model Fit Statistics:\n",
    "- R-squared: The coefficient of determination, representing the proportion of variance explained by the model. Higher values closer to 1 indicate better fit.\n",
    "- Adj. R-squared: R-squared adjusted for the number of predictors in the model.\n",
    "    - The standard R-squared (R²) measures the proportion of variance in the dependent variable (target) explained by the independent variables in the model. However, as more predictors are added to the model, R-squared may tend to increase, even if the added variables do not significantly improve the model's explanatory power. This increase might happen due to overfitting, where the model fits the noise in the data rather than capturing the underlying relationships.\n",
    "- F-statistic: The F-statistic for the overall significance of the model.\n",
    "    -  A larger F-statistic with a small associated p-value (usually less than 0.05) indicates that the overall model is statistically significant. This suggests that the included predictors collectively have a significant impact on explaining the variation in the dependent variable.\n",
    "- Prob (F-statistic): The p-value associated with the F-statistic.\n",
    "- log-likelihood: It is not directly applicable in standard linear regression analyses based on Ordinary Least Squares. It becomes more relevant in models involving likelihood-based estimation techniques or other models where explicit probability distributions are specified for the dependent variable.\n",
    "- AIC (Akaike Information Criterion): measure used to estimate the relative quality of statistical models. It seeks to balance the goodness of fit of the model with its simplicity or parsimony.\n",
    "    - Lower AIC values indicate better-fitting models relative to higher AIC values. The model with the lowest AIC among competing models is generally preferred.\n",
    "- BIC (Bayesian Information Criterion): similar to AIC, is used for model selection and balancing goodness of fit with model complexity.\n",
    "    - lower BIC values indicate better-fitting models. BIC places a higher penalty on models with more parameters compared to AIC, favoring simpler models even more strongly.\n",
    "\n",
    "Residuals:\n",
    "-  Omnibus Test: is a statistical test used in regression analysis to evaluate the overall significance of a regression model by testing whether at least one of the independent variables significantly contributes to explaining the variance in the dependent variable.\n",
    "    -  A significant result (small p-value, usually less than the chosen significance level, such as 0.05) from the Omnibus test indicates that at least one of the predictors contributes significantly to explaining the variance in the dependent variable. In other words, it suggests that the overall regression model is statistically significant.\n",
    "- Skewness: Measure of the symmetry of the distribution of residuals around zero. residuals is the differences between observed and predicted values.\n",
    "    - If the residuals are approximately normally distributed, the skewness should be close to zero, indicating symmetric distribution around the mean. However, significant departure from zero might suggest non-normality or the presence of outliers in the data.\n",
    "- Kurtosis: a statistical measure that describes the peakedness or flatness of the distribution of a dataset, particularly with respect to the tails of the distribution. In the context of regression analysis, assessing the kurtosis of residuals helps in understanding the shape and characteristics of the distribution of the residuals.\n",
    "    - A kurtosis value of 3 is often considered as the kurtosis of a normal distribution. Values greater than 3 indicate heavier tails (more peaked) than the normal distribution, while values less than 3 indicate lighter tails (less peaked) than the normal distribution.\n",
    "\n",
    "Other Information:\n",
    "- Autocorrelation of residuals: refers to the presence of a systematic pattern or correlation among the residuals (errors) of a regression model at different time points or observations. In simpler terms, it implies that the residuals are not independent and exhibit a predictable relationship with each other across the dataset.\n",
    "- Durbin-Watson: Test for autocorrelation in the residuals (values between 0 and 4; closer to 2 suggests no autocorrelation).\n",
    "- Jarque-Bera (JB) and Prob(JB): It evaluates whether the residuals (errors) from a regression analysis follow a normal distribution based on their skewness and kurtosis.\n",
    "    - A low p-value (typically less than the chosen significance level, e.g., 0.05) suggests evidence against the null hypothesis, indicating that the residuals might not follow a normal distribution. Higher p-values indicate no significant departure from normality.\n",
    "- Cond. No.: Condition number for multicollinearity diagnosis is a measure used in linear regression models to diagnose multicollinearity among the predictor variables. Multicollinearity occurs when there are high correlations among the independent variables in a regression model, leading to issues in the estimation of coefficients and the interpretation of the model.\n",
    "    - A high Condition Number indicates multicollinearity issues, suggesting that the independent variables are highly correlated.\n",
    "\n",
    "Interpreting:\n",
    "1. Coefficient Significance: Look at the p-values (< 0.05 generally indicates significance). Significant coefficients suggest a stronger relationship with the dependent variable.\n",
    "2. R-squared: Measures the goodness-of-fit. Higher values indicate a better fit, but be cautious of overfitting.\n",
    "3. Residual Analysis: Check for normality, homoscedasticity, and independence of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "\n",
    "# we usually have y from collected data, thus we don't know the intercept and the coefficient.\n",
    "y = 3 + 4 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add constant term for intercept in statsmodels\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary statistics\n",
    "print(model.summary())\n",
    "\n",
    "# Plotting the data and regression line\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], model.predict(X), color='red', linewidth=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression with Statsmodels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f534ac",
   "metadata": {},
   "source": [
    "> Using `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 3 + 4 * X + 2*np.random.randn(100, 1)\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print('Coefficients:', model.coef_)\n",
    "print('Intercept:', model.intercept_)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Plotting the data and regression line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, predictions, color='red', linewidth=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression with Scikit-learn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978ac72",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Exercise:</b> \n",
    "\n",
    "1. Is there a relationship between `petal_length` & `petal_width` of iris dataset? \n",
    "2. Can you predict the `petal_width` based on `petal_length`?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d84d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant term for intercept in statsmodels\n",
    "X = sm.add_constant(iris[\"petal_length\"])\n",
    "y = iris[\"petal_width\"]\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary statistics\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3187ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(iris, hue=\"species\").\\\n",
    "    map(plt.scatter, \"petal_length\", \"petal_width\").\\\n",
    "        add_legend()\n",
    "\n",
    "# Plotting the data and regression line\n",
    "#plt.scatter(X['petal_length'], y)\n",
    "plt.plot(X['petal_length'], model.predict(X), color='black', linewidth=3)\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.title('Linear Regression: Petal length Vs. Petal Width')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e65049",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8615502",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['petal_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f603ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(iris, hue=\"species\").\\\n",
    "    map(plt.scatter, \"petal_length\", \"petal_width\").\\\n",
    "        add_legend()\n",
    "\n",
    "# Plotting the data and regression line\n",
    "#plt.scatter(X['petal_length'], y)\n",
    "plt.plot(X['petal_length'], model.predict(X), color='black', linewidth=3)\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.title('Linear Regression: Predict Petal Width')\n",
    "\n",
    "predict_x = 6.5\n",
    "plt.scatter(predict_x , model.predict([1, predict_x ]),color='magenta')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a1746",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Exercise:</b> Can you predict the `petal_length` based on `sepal_length`? Use a linear model regression, and explain how do it is. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99268567",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60e8e9",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Multiple Regression\n",
    "\n",
    "\n",
    "Linear regression can be extended to multiple independent variables, known as multiple linear regression. The equation for multiple linear regression is:\n",
    "\n",
    "$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n $\n",
    "\n",
    "Where:\n",
    "- $ y $ is the dependent variable.\n",
    "- $ x_1, x_2, \\ldots, x_n $ are the independent variables.\n",
    "- $ \\beta_0 $ is the y-intercept.\n",
    "- $ \\beta_1, \\beta_2, \\ldots, \\beta_n $ are the coefficients (slopes) associated with each independent variable.\n",
    "\n",
    "- Multiple linear regression allows the investigator to account for all of these potentially important factors in one model. \n",
    "- The advantages of this approach are that this may lead to a more accurate and precise understanding of the association of each individual factor with the outcome. \n",
    "- It also yields an understanding of the association of all the factors as a whole with the outcome, and the associations between the various predictor variables themselves.\n",
    "\n",
    "**Addtion assumption to be met**: \n",
    "- No Multicollinearity; In multiple linear regression, the independent variables **should not be highly correlated** with each other. High multicollinearity can lead to issues in interpreting the coefficients and can affect the model's stability.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfd5cd",
   "metadata": {},
   "source": [
    "> toy example 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'X2': [3, -4, 2, 5, 10, -6, 8, -3, 10, 14],\n",
    "    'Y': [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "}\n",
    "\n",
    "# Creating a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables (X) and dependent variable (Y)\n",
    "X = df[['X1', 'X2']]\n",
    "Y = df['Y']\n",
    "\n",
    "# Adding a constant for the intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot colored by 'Y' (height)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(df['X1'], df['X2'], df['Y'], c=df['Y'], cmap='viridis')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Y (Height)')\n",
    "ax.set_title('3D Scatter Plot with Matplotlib')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Y (Height)', rotation=270, labelpad=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dd1d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Elaborate the results:</b>  Does this multiple regression need X2 predictor in the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dce61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New values of X1 and X2 for prediction\n",
    "new_X1 = 5\n",
    "new_X2 = 12\n",
    "\n",
    "# Predict for the new values\n",
    "new_data = sm.add_constant([[1, new_X1, new_X2]])  # Add a constant and prepare the new data correctly\n",
    "predicted_Y = model.predict(new_data)\n",
    "\n",
    "print(f\"Predicted Y for X1={new_X1} and X2={new_X2}: {predicted_Y[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot colored by 'Y' (height)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(df['X1'], df['X2'], df['Y'], c=df['Y'], cmap='Blues')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Y (Height)')\n",
    "ax.set_title('3D Scatter Plot with Matplotlib')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Y (Height)', rotation=270, labelpad=15)\n",
    "\n",
    "\n",
    "# Scatter plot for predicted value\n",
    "\n",
    "ax.scatter(new_X1, new_X2, predicted_Y, color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Original scatter plot\n",
    "fig = px.scatter_3d(df, x='X1', y='X2', z='Y', color='Y', color_discrete_sequence=['blue'])\n",
    "\n",
    "# predicted scatter plot\n",
    "\n",
    "new_point = px.scatter_3d(x=[new_X1], y=[new_X2], z=predicted_Y, color =[100] )\n",
    "fig.add_trace(new_point.data[0])\n",
    "# Update the marker style for the added point to 'box'\n",
    "fig.update_traces()\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d348d",
   "metadata": {},
   "source": [
    "- 1D: line\n",
    "- 2D: plane \n",
    "- 3D: Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c9eff",
   "metadata": {},
   "source": [
    "> toy example 2 : strong multicollinearity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b843645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'X1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'X2': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30],\n",
    "    'Y': [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "}\n",
    "\n",
    "# Creating a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables (X) and dependent variable (Y)\n",
    "X = df[['X1', 'X2']]\n",
    "Y = df['Y']\n",
    "\n",
    "# Adding a constant for the intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f46b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Original scatter plot\n",
    "fig = px.scatter_3d(df, x='X1', y='X2', z='Y', color='Y', color_discrete_sequence=['red'])\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7aae0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Exercise:</b> \n",
    "\n",
    "1. Is there a relationship between water `petal_length` & `petal_width` of iris dataset? \n",
    "2. Can you predict the `petal_width` based on `petal_length`?\n",
    "3. Construct your arguments based on your statistical and visualization outcome.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e83b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9da6e",
   "metadata": {},
   "source": [
    "> More examples: Diamond dataset\n",
    "\n",
    "- The term \"carat\" is a unit of measurement used to describe the weight of diamonds and other gemstones. One carat is equivalent to 200 milligrams or 0.2 grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv using pandas\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/diamond.csv')\n",
    "\n",
    "# check the head of dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf698c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Carat Weight', 'Price']].corr(method='kendall')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4432c9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Exercise:</b> Can you predict the `price` of the diamond based on `carat weight`?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = data['Carat Weight']\n",
    "# we usually have y from collected data, thus we don't know the intercept and the coefficient.\n",
    "y = data['Price']\n",
    "\n",
    "# Add constant term for intercept in statsmodels\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary statistics\n",
    "print(model.summary())\n",
    "\n",
    "# Plotting the data and regression line\n",
    "plt.scatter(X['Carat Weight'], y)\n",
    "plt.plot(X['Carat Weight'], model.predict(X), color='black', linewidth=3)\n",
    "plt.xlabel('Carat Weight')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Linear Regression with Statsmodels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59230169",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Info:</b>  \n",
    "The carat weight of a diamond is a significant factor in determining its value and size. However, it's important to note that carat weight alone doesn't solely determine a diamond's worth. Other factors such as cut, color, and clarity also play crucial roles in evaluating a diamond's overall quality and value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046fe74",
   "metadata": {},
   "source": [
    "> Convert categorical dataset into orders: In data science, the process of mapping categorical or qualitative data (such as labels, categories, or text) to numerical representations is often referred to as \"Encoding.\" This process is essential for machine learning algorithms that require numerical input because these algorithms typically cannot directly process categorical data.\n",
    "\n",
    "- In regression, the goal is to predict a continuous numeric value (the dependent variable) based on one or more independent variables (features). While regression models work with numeric data, they might encounter categorical variables that need to be converted into numerical representations.\n",
    "\n",
    "- Encoding categorical variables becomes essential when performing regression analysis involving features that are not inherently numeric. By converting categorical variables into numerical representations, regression models can handle these variables as part of the input features.\n",
    "\n",
    "- **Ordinal Encoding**: maps categorical values to ordered integers. This might be suitable when the categorical variable has an inherent order or ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308063fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Cut\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846cac9",
   "metadata": {},
   "source": [
    "> check the new variables' classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b951201",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"dark\")\n",
    "\n",
    "# Draw a nested violinplot and split the violins for easier comparison\n",
    "sns.violinplot(data=data, x=\"Cut\", y=\"Price\", fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bf8d2",
   "metadata": {},
   "source": [
    "1. Price, doesn't really relate to type of cut, but it has a 'rough' ranking (lowest to highest max price): {fair,good, signature-Ideal, Very Good, Ideal}\n",
    "2. all are skewed classes with outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping diamond qualities to integers using a dictionary\n",
    "quality_to_int = {\n",
    "    'Ideal': 5,\n",
    "    'Very Good': 4,\n",
    "    'Signature-Ideal': 3,\n",
    "    'Good': 2,\n",
    "    'Fair': 1,\n",
    "}\n",
    "\n",
    "# Applying map function to the DataFrame column\n",
    "data['Mapped Cut'] = data['Cut'].map(quality_to_int)\n",
    "\n",
    "new_quantified_diamond_data = data[['Carat Weight', 'Mapped Cut', 'Price']]\n",
    "new_quantified_diamond_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8b2c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Exercise:</b> Now predict the `price` of the diamond based on `Carat Weight` and `Mapped Cut`? Does the model gets better? Explain.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089836b8",
   "metadata": {},
   "source": [
    ">Check correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Carat Weight', 'Mapped Cut']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66599068",
   "metadata": {},
   "source": [
    "Mapped Cut is a skewed variable as compared to Carrat Weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cddb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = new_quantified_diamond_data.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f60fa",
   "metadata": {},
   "source": [
    "1. Good sign: Carat weight is not related to Mapped Cut. (there won't be multicollinearity problem between this two independent variable)\n",
    "2. Warning: Mapped Cut is not related to Price!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449287c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a pandas DataFrame\n",
    "df = pd.DataFrame(new_quantified_diamond_data)\n",
    "\n",
    "# Independent variables (X) and dependent variable (Y)\n",
    "X = df[['Carat Weight', 'Mapped Cut']]\n",
    "Y = df['Price']\n",
    "\n",
    "# Adding a constant for the intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acc3bf",
   "metadata": {},
   "source": [
    "> Inference from the multiple regression model:\n",
    "1. Very little increase R-squared:0.739, slightly better than the previous simple regression model; R-squared:0.737.\n",
    "2. new variable (Mapped cut) is significant in this model where $\\beta_2=409.2708$, pvalue 0.00 < 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911798d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814da96",
   "metadata": {},
   "source": [
    "# Bonus: Using Seaborn for Regression Visualization\n",
    "- https://seaborn.pydata.org/examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Make an example dataset with y ~ x\n",
    "rs = np.random.RandomState(7)\n",
    "x = rs.normal(2, 1, 75)\n",
    "y = 2 + 1.5 * x + rs.normal(0, 2, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b37d4b",
   "metadata": {},
   "source": [
    "> using `statsmodels` for regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Independent variables (X) and dependent variable (Y)\n",
    "X = x\n",
    "Y = y\n",
    "\n",
    "# Adding a constant for the intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the multiple linear regression model\n",
    "model = sm.OLS(Y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1bfe1",
   "metadata": {},
   "source": [
    "> directly using `scipy` for regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "# Adding a constant for the intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate linear regression statistics\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Print regression statistics\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Standard Error: {std_err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c7e93",
   "metadata": {},
   "source": [
    "> directly using seaborn for regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a666aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=x, y=y, \n",
    "                  kind=\"reg\", truncate=True,\n",
    "                  color=\"b\", height=4)\n",
    "# Display R-squared value\n",
    "r_squared = r_value**2\n",
    "plt.text(0.5, 9, f'R-squared: {r_squared:.2f}',  fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df95f5e",
   "metadata": {},
   "source": [
    "> In Seaborn's residplot, lowess stands for Locally Weighted Scatterplot Smoothing. \n",
    "\n",
    "- It is a method used to create a smooth line that shows the relationship between two variables, typically the relationship **between the predictor variable and the residual (difference between observed and predicted values)** in a regression analysis.\n",
    "\n",
    "- Adding a LOWESS curve can help reveal or emphasize structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals after fitting a linear model\n",
    "sns.residplot(x=x, y=y, lowess=True, color=\"b\", line_kws=dict(color=\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "g = sns.jointplot(x='Carat Weight', y='Price', data=data,\n",
    "                  kind=\"reg\", truncate=False,\n",
    "                   line_kws=dict(color=\"y\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1eda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Model setting Price ~ Carat Weight\n",
    "x=data['Carat Weight']\n",
    "y=data['Price']\n",
    "\n",
    "# Plot the residuals after fitting a linear model\n",
    "sns.residplot(x=x, y=y, lowess=True, color=\"g\",line_kws=dict(color=\"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b3dd6",
   "metadata": {},
   "source": [
    "> using `tips` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a435d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "g = sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips,\n",
    "                  kind=\"reg\", truncate=False,\n",
    "                  xlim=(0, 60), ylim=(0, 12),\n",
    "                  color=\"b\", height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ccf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an example dataset with y ~ x\n",
    "x=tips[\"total_bill\"]\n",
    "y=tips[\"tip\"]\n",
    "# Plot the residuals after fitting a linear model\n",
    "sns.residplot(x=x, y=y, lowess=True, color=\"g\",line_kws=dict(color=\"r\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
